---
title: ''VQA with BLIP-2 for Self-Driving Cars'
date: 2025-06-05
permalink: /posts/2025/06/blog-post-1/
tags:
  - VQM
  - VLM
  - self-driving cars
---

# Vision-Language Models (VLMs) and Vision-Language Agents (VLAs) in Self-Driving Cars

## Introduction

Autonomous driving systems are evolving to include not just sensor-based perception but also reasoning and interaction. Recent advancements in AI have introduced **Vision-Language Models (VLMs)** and **Vision-Language Agents (VLAs)** capable of interpreting scenes, answering questions, and making explainable decisions.

## What Are VLMs?

**Vision-Language Models (VLMs)** are neural networks that jointly process visual inputs (e.g., images, videos) and language (e.g., text, commands). Tasks include:

- Visual Question Answering (VQA)
- Image Captioning
- Referring Expressions
- Multimodal Classification

**Examples:**
- [CLIP](https://github.com/openai/CLIP)
- [BLIP-2](https://github.com/salesforce/BLIP)
- GPT-4V (Multimodal GPT)


## Applications of VLMs in Self-Driving Cars

- **Scene Captioning**: `"A pedestrian is crossing at the intersection."`
- **VQA**: `"Is the light red?" → Yes.`
- **Command Grounding**: `"Stop near the yellow barrier."`
- **Annotation Automation**: Describe frames with metadata for training.


## What Are VLAs?

**Vision-Language Agents (VLAs)** are goal-directed systems built on VLMs. They maintain memory, reason over time, and interact with users and their environment.

Key abilities:
- Multimodal reasoning
- Task planning and memory
- Dialogue and explanation
- Adaptive interaction


## Applications of VLAs in Autonomous Driving

- **Voice-Driven Assistants**: `"Take the second right after the truck."`
- **Explainability Agents**: `"Why did you stop?" → Because a cyclist was nearby.`
- **Fleet Control Interfaces**: Natural-language summaries of road conditions
- **Safety Monitors**: Question-answering based on visual inputs


##  Architecture Diagrams

### Vision-Language Model (VLM)

Image → [Vision Encoder]
Text → [Text Encoder]
↓
[Multimodal Fusion Layer]
↓
Joint Vision-Language Features


### Vision-Language Agent (VLA)
```sql
      +----------------+
      | Sensor Inputs  |
      +--------+-------+
               ↓
    +----------+------------+
    |  Vision-Language Model |
    +----------+------------+
               ↓
   +-----------+-----------+
   |  Reasoning / Memory   |
   +-----------+-----------+
               ↓
    Action / Explanation Output
```
## Challenges and Considerations


| Challenge             | Description                                               |
|----------------------|-----------------------------------------------------------|
| Latency              | Real-time processing demands fast multimodal inference    |
| Reliability          | Needs to generalize across weather, lighting, and events  |
| Ambiguity in Language| Commands like "near the tree" must be grounded accurately |
| Safety               | Language must not override critical safety protocols      |


## Step 1: Install Required Libraries

```bash
pip install torch torchvision transformers
pip install git+https://github.com/salesforce/BLIP.git
```
## Step 2: Load the BLIP-2 Model
```python
import torch
from transformers import Blip2Processor, Blip2ForConditionalGeneration
from PIL import Image

# Load model and processor
device = "cuda" if torch.cuda.is_available() else "cpu"
processor = Blip2Processor.from_pretrained("Salesforce/blip2-opt-2.7b")
model = Blip2ForConditionalGeneration.from_pretrained(
    "Salesforce/blip2-opt-2.7b",
    torch_dtype=torch.float16 if device == "cuda" else torch.float32
).to(device)
```
## Step 3: Load the Road Scene Image
```python
image = Image.open("road_scene.jpg").convert("RGB")
```
## Step 4: Ask Driving-Relevant Questions
```python
questions = [
    "Is there a stop sign?",
    "Are any pedestrians crossing the street?",
    "What color is the traffic light?",
    "What is the vehicle in front doing?",
    "Is it safe to turn left?"
]

for question in questions:
    inputs = processor(images=image, text=question, return_tensors="pt").to(
        device, torch.float16 if device == "cuda" else torch.float32
    )
    output = model.generate(**inputs, max_new_tokens=50)
    answer = processor.decode(output[0], skip_special_tokens=True)
    print(f"Q: {question}\\nA: {answer}\\n")
```
## Bonus: Scene Descriptio
```python
inputs = processor(images=image, text="Describe the scene.", return_tensors="pt").to(
    device, torch.float16 if device == "cuda" else torch.float32
)
output = model.generate(**inputs, max_new_tokens=50)
description = processor.decode(output[0], skip_special_tokens=True)
print("Scene Description:", description)
```
## Example Output
```vbnet
Q: Is there a stop sign?
A: Yes, there is a stop sign on the right side of the street.

Q: Are any pedestrians crossing the street?
A: Yes, one pedestrian is crossing at the crosswalk.

Q: What color is the traffic light?
A: The traffic light is red.

Q: What is the vehicle in front doing?
A: The vehicle in front is braking.

Q: Is it safe to turn left?
A: No, a car is approaching from the opposite direction.

```
## References

- [OpenAI CLIP](https://github.com/openai/CLIP)
- [BLIP-2](https://github.com/salesforce/BLIP)
- [LLaVA](https://github.com/haotian-liu/LLaVA)
- [GPT-4V](https://openai.com/gpt-4)
- [CARLA Simulator](http://carla.org/)
