{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db8df1ac",
   "metadata": {},
   "source": [
    "# üöò BLIP-2 Vision-Language VQA Demo for Self-Driving Cars\n",
    "This notebook demonstrates how to use BLIP-2 to answer questions about road scenes. Perfect for developing interpretable vision-language agents for autonomous vehicles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dcaff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Install Dependencies\n",
    "!pip install torch torchvision transformers\n",
    "!pip install git+https://github.com/salesforce/BLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa747ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† Load BLIP-2 Model and Processor\n",
    "import torch\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/blip2-opt-2.7b\",\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e99560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üñºÔ∏è Load Image\n",
    "# Replace 'road_scene.jpg' with your own image\n",
    "image = Image.open(\"road_scene.jpg\").convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d69c0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùì Ask Driving-Relevant Questions\n",
    "questions = [\n",
    "    \"Is there a stop sign?\",\n",
    "    \"Are any pedestrians crossing the street?\",\n",
    "    \"What color is the traffic light?\",\n",
    "    \"What is the vehicle in front doing?\",\n",
    "    \"Is it safe to turn left?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    inputs = processor(images=image, text=question, return_tensors=\"pt\").to(\n",
    "        device, torch.float16 if device == \"cuda\" else torch.float32\n",
    "    )\n",
    "    output = model.generate(**inputs, max_new_tokens=50)\n",
    "    answer = processor.decode(output[0], skip_special_tokens=True)\n",
    "    print(f\"Q: {question}\\nA: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6976740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† Bonus: Describe the Scene\n",
    "inputs = processor(images=image, text=\"Describe the scene.\", return_tensors=\"pt\").to(\n",
    "    device, torch.float16 if device == \"cuda\" else torch.float32\n",
    ")\n",
    "output = model.generate(**inputs, max_new_tokens=50)\n",
    "description = processor.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Scene Description:\", description)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
